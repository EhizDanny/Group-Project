{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('LoanPred-Dataset.csv', skiprows = 1) #skiprows skips the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Unnamed: 0', 'ID'], axis = 1, inplace= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PAY_0'].value_counts()\n",
    "# df['PAY_0'].unique()\n",
    "# by running this,we have 14737 values that has '0' which isnt specified in the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PAY_2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for all the columns\n",
    "{column: len(df[column].unique()) for column in df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['SEX'])\n",
    "# more female took loan than male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['EDUCATION'])\n",
    "# more university grauduate took loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['MARRIAGE'])\n",
    "# single people took more loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['default payment next month'])\n",
    "# few people default payment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the correlation of the data\n",
    "\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(corr, annot = True, cmap= 'BuPu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIll the null values with zero\n",
    "for column in ['BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']:\n",
    "    df[column]= df[column].fillna(0)\n",
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a column to cover the bill amount columns\n",
    "df['total_Bill_Amt']= (df['BILL_AMT1'] + df['BILL_AMT2'] + df['BILL_AMT3'] + \n",
    "                        df['BILL_AMT4'] + df['BILL_AMT5'] + df['BILL_AMT6'])\n",
    "\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing selected variable (selVar)\n",
    "selVar = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',\n",
    "              'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4',  'PAY_AMT5', 'PAY_AMT6', \n",
    "                        'LIMIT_BAL', 'SEX', 'EDUCATION', 'AGE', 'MARRIAGE']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we need to keep a slice of data from the entire model selection and training process, and save it for the final evaluation. This slice of data is the “final exam” to our model and the exam questions must not be seen by the model before. Precisely, this is the workflow of how the data is being used:\n",
    "\n",
    "1. training dataset is used to train a few candidate models\n",
    "2. validation dataset is used to evaluate the candidate models\n",
    "3. one of the candidates is chosen\n",
    "4. the chosen model is trained with a new training dataset\n",
    "5. the trained model is evaluated with the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1',\n",
       "       'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
       "       'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
       "       'LIMIT_BAL', 'SEX', 'EDUCATION', 'AGE', 'MARRIAGE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[selVar]\n",
    "y =df['default payment next month']\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardaize X with standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler() # instatiate the scaler\n",
    "# X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns) # all X variables will have a mean of 0 and a unique variance\n",
    "# print (X)\n",
    "\n",
    "# OR\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "Standardize = scaler.transform(X) # this standardize the data and make all values fall between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 23) (24000,) (6000, 23) (6000,)\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test, for both features and target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify = y)\n",
    "print (X_train.shape, y_train.shape, X_test.shape,  y_test.shape ) # so to confirm the sharing formular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20400, 23) (20400,) (3600, 23) (3600,)\n"
     ]
    }
   ],
   "source": [
    "# Further split the training data into train and the validation\n",
    " \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=100) # 0.25 x 0.8 = 0.2\n",
    "print (X_train.shape, y_train.shape, X_val.shape,  y_val.shape ) # so to confirm the sharing formular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and train the model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "loan_model = LogisticRegression()\n",
    "loan_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation prediction:  [0 0 0 ... 0 0 0]\n",
      "initial default nextmonth:  0        1\n",
      "1        1\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "29995    0\n",
      "29996    0\n",
      "29997    1\n",
      "29998    1\n",
      "29999    1\n",
      "Name: default payment next month, Length: 30000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Evaluate or Validated your prediction  \n",
    "val_prediction = loan_model.predict(X_val)\n",
    "print('validation prediction: ', val_prediction) # print out top few validated predictions\n",
    "print('initial default nextmonth: ', y)   # print the top few actual prices from validation data and check it against validated prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute error:  0.215\n"
     ]
    }
   ],
   "source": [
    "# Compute the Mean Absolute Error of the model, if Satisfied, then proceed to testing the model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "val_mae = mean_absolute_error(y_val, val_prediction)\n",
    "print('mean absolute error: ', val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once we are satisfied with the mean absolute error of the model, \n",
    "# we proceed to train the Logistic Regression model with the full training data  (X,y)\n",
    "# to improve the accuracy of the model\n",
    "# And test with our reserved testing datasets\n",
    "\n",
    "loan_model_FT = LogisticRegression()\n",
    "loan_model_FT.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prediction:  [0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Retraining the model and predicting wiht full data X and y \n",
    "\n",
    "test_prediction = loan_model_FT.predict(X_test) # instatiate the model\n",
    "print ('Final Prediction: ', test_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of the final prediction:  0.22116666666666668\n"
     ]
    }
   ],
   "source": [
    "# Checking the mean absolute error of the model\n",
    "\n",
    "test_mae = mean_absolute_error(y_test, test_prediction)\n",
    "print('Mean Absolute Error of the final prediction: ', test_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Predictive System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.06782215e+05  1.78234817e+00  1.80992130e+00  1.89943574e+00\n",
      "   2.31794297e+01 -6.16451690e-01 -6.95614667e-01 -6.90983429e-01\n",
      "  -6.77928676e-01 -6.72497269e-01 -6.63025635e-01 -6.07688790e-01\n",
      "  -2.37793846e-01 -1.40585726e-01 -1.10962591e-01 -8.75198352e-02\n",
      "  -1.00692662e-01 -2.93382058e-01 -1.28316068e+00  2.04120489e+03\n",
      "   1.26294072e+03 -3.84969199e+00  3.82873148e+03]]\n",
      "Test prediction: [1]\n",
      " The person is diabetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:441: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:441: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# We take a data from the original raw dataset, run it on the model to test if its output correspond with the Y\n",
    "input = (120000,2,2,2,26,-1,2,0,0,0,2,2682,1725,2682,3272,3455,3261,0,1000,1000,1000,0,2000) # Note that we didnt include Y column.capitalize\n",
    "\n",
    "# We need to change the input to numpy array\n",
    "input_numpy = np.asarray(input)\n",
    "\n",
    "# The model was trained with 30,000 dataset, so its expects 30,000 dataset to run\n",
    "# We then need to reshape so that a single dataset can run\n",
    "input_reshape =  input_numpy.reshape(1, -1)\n",
    "\n",
    "# We cannot use raw data to test a mpdel, we need to standardize it. Hence we Standardize the input\n",
    "final_input = scaler.transform(input_reshape)\n",
    "print (final_input)\n",
    "\n",
    "# Now predict what the label Y will be \n",
    "prediction = loan_model_FT.predict(final_input)\n",
    "print('Test prediction:', prediction)\n",
    "\n",
    "if (prediction [0] == 0):\n",
    "    print ('The person is not diabetic')\n",
    "else:\n",
    "    print(' The person is diabetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c569cd2e17f62341e0f08a54f9a867c3c0d3a6f67454072d7de41a8b5dff8343"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
